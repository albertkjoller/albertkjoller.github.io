---
---

@article{gegenfurtner2025reducing,
      title={Reducing Memorisation in Generative Models via Riemannian Bayesian Inference},
      author={Gegenfurtner*, Johanna Marie and <b><u>Jacobsen*</u></b>, <b><u>Albert Kj{\o}ller </u></b> and Arvanitidis, Georgios},
      journal={EurIPS - Workshop on Principles of Generative Modeling (PriGM)},
      selected={true},
      abbr={EurIPS},
      year={2025},
      preview={riemann-laplace-V2.png},
      venue={EurIPS},
      pdf={https://openreview.net/pdf?id=fN0iGWIqPy},
      abstract={How to balance memorisation and generalisation in generative models remains an open task. To investigate this, we employ Bayesian methods, which have recently been proposed to predict the uncertainty of generated samples. In our work, we employ the Riemannian Laplace approximation, from which we can sample generative models that resemble the trained one. Our geometry-aware approach yields improved results compared to the Euclidean counterpart.},
      poster={poster_prigm.pdf},
}

@article{jacobsen2025stayingmanifoldgeometryawarenoise,
      code={https://github.com/albertkjoller/geometric-ml/tree/main/staying-on-the-manifold},
      selected={true},
      abbr={NLDL},
      title={Staying on the Manifold: Geometry-Aware Noise Injection}, 
      author={<b><u>Jacobsen*</u></b>, <b><u>Albert Kj{\o}ller </u></b> and Gegenfurtner*, Johanna Marie and Arvanitidis, Georgios},
      journal={Northern Lights Deep Learning Conference (NLDL)},
      year={2026},
      preview={mnist_noise_animation.gif},
      venue={NLDL},
      pdf={https://openreview.net/pdf?id=GRk65sJxpn},
      abstract={It has been shown that perturbing the input during training implicitly regularises the gradient of the learnt function, leading to smoother models and enhancing generalisation. However, previous research mostly considered the addition of ambient noise in the input space, without considering the underlying structure of the data. In this work, we propose several methods of adding geometry-aware input noise that accounts for the lower dimensional manifold the input space inhabits. We start by projecting ambient Gaussian noise onto the tangent space of the manifold. In a second step, the noise sample is mapped on the manifold via the associated geodesic curve. We also consider Brownian motion noise, which moves in random steps along the manifold. We show that geometry-aware noise leads to improved generalization and robustness to hyperparameter selection on highly curved manifolds, while performing at least as well as training without noise on simpler manifolds. Our proposed framework extends to learned data manifolds.},
      poster={poster_staying_on_manifold.pdf}, 
}

@article{jacobsen2025monge,
  selected={true},
  abbr={Preprint},
  title={Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware Minimization Based on Loss Geometry},
  author={<b><u>Jacobsen</u></b>, <b><u>Albert Kj{\o}ller </u></b> and Arvanitidis, Georgios},
  journal={arXiv preprint},
  year={2025},
  preview={mongesam.png},
  venue={Preprint},
  pdf={https://arxiv.org/pdf/2502.08448},
  abstract={Recent studies on deep neural networks show that flat minima of the loss landscape correlate with improved generalization. Sharpness-aware minimization (SAM) efficiently finds flat regions by updating the parameters according to the gradient at an adversarial perturbation. The perturbation depends on the Euclidean metric, making SAM non-invariant under reparametrizations, which blurs sharpness and generalization. We propose Monge SAM (M-SAM), a reparametrization invariant version of SAM by considering a Riemannian metric in the parameter space induced naturally by the loss surface. Compared to previous approaches, M-SAM works under any modeling choice, relies only on mild assumptions while being as computationally efficient as SAM. We theoretically argue that M-SAM varies between SAM and gradient descent (GD), which increases robustness to hyperparameter selection and reduces attraction to suboptimal equilibria like saddle points. We demonstrate this behavior both theoretically and empirically on a multi-modal representation alignment task.},
}

@article{dorszewski2025redundant,
  code={https://github.com/albertkjoller/transformer-redundancy},
  selected={true},
  title={How Redundant Is the Transformer Stack in Speech Representation Models?},
  author={Dorszewski*, Teresa and <b><u>Jacobsen*</u></b>, <b><u>Albert Kj{\o}ller </u></b> and T{\v{e}}tkov{\'a}, Lenka and Hansen, Lars Kai},
  journal={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE},
  abbr={ICASSP},
  preview={redundancy-V2.png},
  pdf={https://ieeexplore.ieee.org/document/10887866},
  abstract={Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the modelâ€™s predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size by 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.},
  poster={poster_redundancy.pdf}, 
}